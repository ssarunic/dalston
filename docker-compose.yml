# Dalston - Modular Audio Transcription Server
#
# Container Naming Convention:
#   {type}-{domain}-{stage}-{impl}[-v{version}][-cpu]
#   - type: batch (file processing) or rt (realtime streaming)
#   - domain: stt (speech-to-text), tts (text-to-speech, future)
#   - stage: prepare, transcribe, align, diarize, pii_detect, audio_redact, refine, merge
#   - impl: whisper, parakeet, pyannote, presidio, etc.
#   - version: v31, v40, etc. (only when multiple versions coexist)
#   - cpu: only when forcing CPU mode (GPU is default for compute-heavy engines)
#
# Image Sharing Strategy:
#   Services that use the same Dockerfile share a single image via the `image:` directive.
#   Only the first service has a `build:` section; others just reference the image.
#   This saves disk space and ensures consistency.
#
# Usage:
#   docker compose up -d
#   docker compose up -d --scale stt-batch-transcribe-faster-whisper-base=2

# ============================================================
# EXTENSION FIELDS (shared configurations)
# ============================================================

# Common environment variables for all engines
x-common-env: &common-env
  REDIS_URL: redis://redis:6379
  S3_BUCKET: ${S3_BUCKET:-dalston-artifacts}
  S3_REGION: ${S3_REGION:-us-east-1}
  S3_ENDPOINT_URL: http://minio:9000
  S3_PUBLIC_ENDPOINT_URL: ${S3_PUBLIC_ENDPOINT_URL:-}
  AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minioadmin}
  AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
  LOG_LEVEL: ${LOG_LEVEL:-INFO}
  LOG_FORMAT: ${LOG_FORMAT:-json}

# Observability environment variables
x-observability-env: &observability-env
  OTEL_ENABLED: ${OTEL_ENABLED:-false}
  OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT:-http://jaeger:4317}
  METRICS_ENABLED: ${METRICS_ENABLED:-true}
  METRICS_PORT: 9100

# Core services environment (includes database)
x-core-env: &core-env
  <<: *common-env
  DATABASE_URL: postgresql+asyncpg://dalston:${POSTGRES_PASSWORD:-password}@postgres:5432/dalston
  ENGINE_UNAVAILABLE_BEHAVIOR: ${ENGINE_UNAVAILABLE_BEHAVIOR:-wait}
  ENGINE_WAIT_TIMEOUT_SECONDS: ${ENGINE_WAIT_TIMEOUT_SECONDS:-300}

# Dependencies for batch engines
x-batch-depends: &batch-depends
  redis:
    condition: service_healthy
  minio-init:
    condition: service_completed_successfully

# Dependencies for realtime engines
x-realtime-depends: &realtime-depends
  redis:
    condition: service_healthy

# Dependencies for core services
x-core-depends: &core-depends
  postgres:
    condition: service_healthy
  redis:
    condition: service_healthy
  minio-init:
    condition: service_completed_successfully

# GPU deployment configuration
x-gpu-deploy: &gpu-deploy
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]

# WebSocket healthcheck for realtime engines
x-ws-healthcheck: &ws-healthcheck
  test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.settimeout(2); s.connect(('localhost', 9000)); s.close()"]
  interval: 10s
  timeout: 5s
  retries: 3

services:
  # ============================================================
  # INFRASTRUCTURE
  # ============================================================

  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: dalston
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
      POSTGRES_DB: dalston
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dalston"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    # AOF persistence for durable Streams (M33)
    # - appendonly yes: enables append-only file persistence
    # - appendfsync everysec: fsync every second (balance of durability and performance)
    # - auto-aof-rewrite-percentage 100: rewrite AOF when it doubles in size
    # - auto-aof-rewrite-min-size 64mb: minimum size before rewrite
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --auto-aof-rewrite-percentage 100
      --auto-aof-rewrite-min-size 64mb
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  minio-init:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set dalston http://minio:9000 $${MINIO_ROOT_USER:-minioadmin} $${MINIO_ROOT_PASSWORD:-minioadmin};
      mc mb dalston/$${S3_BUCKET:-dalston-artifacts} --ignore-existing;
      exit 0;
      "
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
      S3_BUCKET: ${S3_BUCKET:-dalston-artifacts}

  # Jaeger for distributed tracing (M19)
  jaeger:
    image: jaegertracing/all-in-one:1.54
    ports:
      - "16686:16686"
      - "4317:4317"
      - "4318:4318"
    environment:
      COLLECTOR_OTLP_ENABLED: "true"
    profiles: [tracing]
    restart: unless-stopped

  # Prometheus for metrics collection (M20)
  prometheus:
    image: prom/prometheus:v2.50.0
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=1d'
      - '--storage.tsdb.retention.size=500MB'
    profiles: [monitoring]
    restart: unless-stopped

  # Grafana for dashboards (M20)
  grafana:
    image: grafana/grafana:10.3.0
    volumes:
      - ./docker/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: dalston
      GF_AUTH_ANONYMOUS_ENABLED: "true"
      GF_AUTH_ANONYMOUS_ORG_ROLE: Viewer
      GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH: /var/lib/grafana/dashboards/dalston-overview.json
    depends_on: [prometheus]
    profiles: [monitoring]
    restart: unless-stopped

  # Metrics exporter for Redis queue depths (M20)
  metrics-exporter:
    image: dalston/gateway:latest
    command: python -m dalston.metrics_exporter
    environment:
      <<: *common-env
      SCRAPE_INTERVAL: 15
    depends_on: *realtime-depends
    profiles: [monitoring]
    restart: unless-stopped

  # ============================================================
  # CORE SERVICES
  # ============================================================

  gateway:
    image: dalston/gateway:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.gateway
    ports:
      - "8000:8000"
    profiles: [prod]
    environment:
      <<: [*core-env, *observability-env]
    depends_on: *core-depends
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  orchestrator:
    image: dalston/orchestrator:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.orchestrator
    profiles: [prod]
    environment:
      <<: [*core-env, *observability-env]
      METRICS_PORT: 8001
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped

  # ============================================================
  # BATCH STT ENGINES
  # ============================================================

  # --- PREPARE STAGE ---
  stt-batch-prepare:
    image: dalston/stt-batch-prepare:latest
    build:
      context: .
      dockerfile: engines/stt-prepare/audio-prepare/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: audio-prepare
    depends_on: *batch-depends
    restart: unless-stopped

  # --- TRANSCRIBE STAGE ---

  # Parakeet CTC 0.6B (GPU - fastest English transcription)
  stt-batch-transcribe-parakeet-ctc-0.6b:
    image: dalston/stt-batch-transcribe-parakeet-ctc-0.6b:1.0.0
    build:
      context: .
      dockerfile: engines/stt-transcribe/parakeet/Dockerfile
      args:
        MODEL_VARIANT: ctc-0.6b
        DEVICE: cuda
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: parakeet-ctc-0.6b
    volumes:
      - parakeet-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # Parakeet CTC 1.1B (GPU - fast English transcription, larger model)
  stt-batch-transcribe-parakeet-ctc-1.1b:
    image: dalston/stt-batch-transcribe-parakeet-ctc-1.1b:1.0.0
    build:
      context: .
      dockerfile: engines/stt-transcribe/parakeet/Dockerfile
      args:
        MODEL_VARIANT: ctc-1.1b
        DEVICE: cuda
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: parakeet-ctc-1.1b
    volumes:
      - parakeet-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [parakeet-ctc-1.1b]
    restart: unless-stopped

  # Parakeet TDT 0.6B v3 (GPU - accurate English transcription)
  stt-batch-transcribe-parakeet-tdt-0.6b-v3:
    image: dalston/stt-batch-transcribe-parakeet-tdt-0.6b-v3:1.0.0
    build:
      context: .
      dockerfile: engines/stt-transcribe/parakeet/Dockerfile
      args:
        MODEL_VARIANT: tdt-0.6b-v3
        DEVICE: cuda
        SKIP_MODEL_DOWNLOAD: "false"
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: parakeet-tdt-0.6b-v3
    volumes:
      - parakeet-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [parakeet-tdt-0.6b-v3]
    restart: unless-stopped

  # Parakeet TDT 0.6B v3 (CPU - for testing without GPU)
  stt-batch-transcribe-parakeet-tdt-0.6b-v3-cpu:
    image: dalston/stt-batch-transcribe-parakeet-tdt-0.6b-v3:1.0.0
    build:
      context: .
      dockerfile: engines/stt-transcribe/parakeet/Dockerfile
      args:
        MODEL_VARIANT: tdt-0.6b-v3
        DEVICE: cpu
        SKIP_MODEL_DOWNLOAD: "true"
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: parakeet-tdt-0.6b-v3
      DEVICE: cpu
    volumes:
      - parakeet-models:/models
    depends_on: *batch-depends
    restart: unless-stopped

  # Parakeet TDT 1.1B (GPU - highest accuracy English transcription)
  stt-batch-transcribe-parakeet-tdt-1.1b:
    image: dalston/stt-batch-transcribe-parakeet-tdt-1.1b:1.0.0
    build:
      context: .
      dockerfile: engines/stt-transcribe/parakeet/Dockerfile
      args:
        MODEL_VARIANT: tdt-1.1b
        DEVICE: cuda
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: parakeet-tdt-1.1b
    volumes:
      - parakeet-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [parakeet-tdt-1.1b]
    restart: unless-stopped

  # Faster Whisper base (CPU/GPU - balanced speed/accuracy)
  stt-batch-transcribe-faster-whisper-base:
    image: dalston/stt-batch-transcribe-faster-whisper-base:1.0.0
    build:
      context: .
      dockerfile: engines/stt-transcribe/faster-whisper/Dockerfile
      args:
        MODEL_VARIANT: base
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: faster-whisper-base
    volumes:
      - whisper-models:/models
    depends_on: *batch-depends
    restart: unless-stopped

  # Faster Whisper large-v3 (GPU - highest accuracy)
  stt-batch-transcribe-faster-whisper-large-v3:
    image: dalston/stt-batch-transcribe-faster-whisper-large-v3:1.0.0
    build:
      context: .
      dockerfile: engines/stt-transcribe/faster-whisper/Dockerfile
      args:
        MODEL_VARIANT: large-v3
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: faster-whisper-large-v3
    volumes:
      - whisper-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # Faster Whisper large-v3-turbo (GPU - fast + accurate)
  stt-batch-transcribe-faster-whisper-large-v3-turbo:
    image: dalston/stt-batch-transcribe-faster-whisper-large-v3-turbo:1.0.0
    build:
      context: .
      dockerfile: engines/stt-transcribe/faster-whisper/Dockerfile
      args:
        MODEL_VARIANT: large-v3-turbo
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: faster-whisper-large-v3-turbo
    volumes:
      - whisper-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [faster-whisper-turbo]
    restart: unless-stopped

  # Voxtral Mini 3B (GPU - multilingual transcription)
  stt-batch-transcribe-voxtral-mini-3b:
    image: dalston/stt-batch-transcribe-voxtral-mini-3b:1.0.0
    build:
      context: .
      dockerfile: engines/stt-transcribe/voxtral/Dockerfile
      args:
        MODEL_VARIANT: mini-3b
        DEVICE: cuda
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: voxtral-mini-3b
    volumes:
      - voxtral-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [voxtral]
    restart: unless-stopped

  # --- ALIGN STAGE ---

  # WhisperX alignment (CPU - default for local dev)
  stt-batch-align-whisperx-cpu:
    image: dalston/stt-batch-align-whisperx:latest
    build:
      context: .
      dockerfile: engines/stt-align/whisperx-align/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: whisperx-align
    volumes:
      - align-models:/models
    depends_on: *batch-depends
    restart: unless-stopped

  # WhisperX alignment (GPU)
  stt-batch-align-whisperx:
    image: dalston/stt-batch-align-whisperx:latest
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: whisperx-align
    volumes:
      - align-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # --- DIARIZE STAGE ---

  # Pyannote 3.1 (CPU - default for local dev)
  stt-batch-diarize-pyannote-3.1-cpu:
    image: dalston/stt-batch-diarize-pyannote-3.1:latest
    build:
      context: .
      dockerfile: engines/stt-diarize/pyannote-3.1/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: pyannote-3.1
      HF_TOKEN: ${HF_TOKEN:-}
      DIARIZATION_DISABLED: ${DIARIZATION_DISABLED:-false}
    volumes:
      - pyannote-models:/models
    depends_on: *batch-depends
    restart: unless-stopped

  # Pyannote 3.1 (GPU)
  stt-batch-diarize-pyannote-3.1:
    image: dalston/stt-batch-diarize-pyannote-3.1:latest
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: pyannote-3.1
      HF_TOKEN: ${HF_TOKEN:-}
    volumes:
      - pyannote-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # Pyannote 4.0 (CPU - default for local dev)
  stt-batch-diarize-pyannote-4.0-cpu:
    image: dalston/stt-batch-diarize-pyannote-4.0:latest
    build:
      context: .
      dockerfile: engines/stt-diarize/pyannote-4.0/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: pyannote-4.0
      HF_TOKEN: ${HF_TOKEN:-}
      DIARIZATION_DISABLED: ${DIARIZATION_DISABLED:-false}
    volumes:
      - pyannote-models:/models
    depends_on: *batch-depends
    restart: unless-stopped

  # Pyannote 4.0 (GPU)
  stt-batch-diarize-pyannote-4.0:
    image: dalston/stt-batch-diarize-pyannote-4.0:latest
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: pyannote-4.0
      HF_TOKEN: ${HF_TOKEN:-}
    volumes:
      - pyannote-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # NeMo MSDD (GPU preferred, CPU allowed with NEMO_ALLOW_CPU=true)
  stt-batch-diarize-nemo-msdd:
    image: dalston/stt-batch-diarize-nemo-msdd:latest
    build:
      context: .
      dockerfile: engines/stt-diarize/nemo-msdd/Dockerfile
      args:
        SKIP_MODEL_DOWNLOAD: ${SKIP_MODEL_DOWNLOAD:-false}
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: nemo-msdd
      DIARIZATION_DISABLED: ${DIARIZATION_DISABLED:-false}
      NEMO_ALLOW_CPU: ${NEMO_ALLOW_CPU:-false}
    volumes:
      - nemo-models:/models
    depends_on: *batch-depends
    profiles: [nemo-msdd]
    restart: unless-stopped

  # --- PII DETECT STAGE ---
  stt-batch-pii-detect-presidio:
    image: dalston/stt-batch-pii-detect-presidio:latest
    build:
      context: .
      dockerfile: engines/stt-detect/pii-presidio/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: pii-presidio
    volumes:
      - gliner-models:/models
    depends_on: *batch-depends
    restart: unless-stopped

  # --- AUDIO REDACT STAGE ---
  stt-batch-audio-redact-audio:
    image: dalston/stt-batch-audio-redact-audio:latest
    build:
      context: .
      dockerfile: engines/stt-redact/audio-redactor/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: audio-redactor
    depends_on: *batch-depends
    restart: unless-stopped

  # --- MERGE STAGE ---
  stt-batch-merge:
    image: dalston/stt-batch-merge:latest
    build:
      context: .
      dockerfile: engines/stt-merge/final-merger/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      ENGINE_ID: final-merger
    depends_on: *batch-depends
    restart: unless-stopped

  # ============================================================
  # REALTIME STT WORKERS
  # ============================================================

  # Parakeet RNNT 0.6B (GPU - default realtime worker, streaming capable)
  stt-rt-transcribe-parakeet-rnnt-0.6b:
    image: dalston/stt-rt-transcribe-parakeet-rnnt-0.6b:1.0.0
    build:
      context: .
      dockerfile: engines/stt-rt/parakeet/Dockerfile
      args:
        MODEL_VARIANT: 0.6b
        DEVICE: cuda
    environment:
      <<: [*common-env, *observability-env]
      MODEL_VARIANT: 0.6b
      WORKER_PORT: 9000
      MAX_SESSIONS: 4
    volumes:
      - parakeet-models:/models
    depends_on: *realtime-depends
    healthcheck: *ws-healthcheck
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # Parakeet RNNT 0.6B CPU (CPU-only realtime worker)
  stt-rt-transcribe-parakeet-rnnt-0.6b-cpu:
    image: dalston/stt-rt-transcribe-parakeet-rnnt-0.6b-cpu:1.0.0
    build:
      context: .
      dockerfile: engines/stt-rt/parakeet/Dockerfile
      args:
        MODEL_VARIANT: 0.6b
        DEVICE: cpu
    environment:
      <<: [*common-env, *observability-env]
      MODEL_VARIANT: 0.6b
      WORKER_PORT: 9000
      MAX_SESSIONS: 4
    volumes:
      - parakeet-models:/models
    depends_on: *realtime-depends
    healthcheck: *ws-healthcheck
    restart: unless-stopped

  # Parakeet RNNT 1.1B (GPU - accurate English streaming)
  stt-rt-transcribe-parakeet-rnnt-1.1b:
    image: dalston/stt-rt-transcribe-parakeet-rnnt-1.1b:1.0.0
    build:
      context: .
      dockerfile: engines/stt-rt/parakeet/Dockerfile
      args:
        MODEL_VARIANT: 1.1b
        DEVICE: cuda
    environment:
      <<: [*common-env, *observability-env]
      MODEL_VARIANT: 1.1b
      WORKER_PORT: 9000
      MAX_SESSIONS: 4
    volumes:
      - parakeet-models:/models
    depends_on: *realtime-depends
    healthcheck: *ws-healthcheck
    deploy: *gpu-deploy
    profiles: [parakeet-rnnt-1.1b]
    restart: unless-stopped

  # Faster Whisper base (GPU)
  stt-rt-transcribe-faster-whisper-base:
    image: dalston/stt-rt-transcribe-faster-whisper-base:1.0.0
    build:
      context: .
      dockerfile: engines/stt-rt/faster-whisper/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      WORKER_PORT: 9000
      MAX_SESSIONS: 4
    volumes:
      - realtime-models:/models
    depends_on: *realtime-depends
    healthcheck: *ws-healthcheck
    deploy: *gpu-deploy
    profiles: [faster-whisper]
    restart: unless-stopped

  # Voxtral Mini 4B (GPU - multilingual realtime)
  stt-rt-transcribe-voxtral-mini-4b:
    image: dalston/stt-rt-transcribe-voxtral-mini-4b:1.0.0
    build:
      context: .
      dockerfile: engines/stt-rt/voxtral/Dockerfile
      args:
        MODEL_VARIANT: mini-4b
    environment:
      <<: [*common-env, *observability-env]
      MODEL_VARIANT: mini-4b
      WORKER_PORT: 9000
      MAX_SESSIONS: 4
      TRANSCRIPTION_DELAY_MS: 480
    volumes:
      - voxtral-models:/models
    depends_on: *realtime-depends
    healthcheck: *ws-healthcheck
    deploy: *gpu-deploy
    profiles: [voxtral]
    restart: unless-stopped

volumes:
  postgres-data:
  redis-data:
  minio-data:
  whisper-models:
  align-models:
  pyannote-models:
  nemo-models:
  realtime-models:
  parakeet-models:
  voxtral-models:
  gliner-models:
  prometheus-data:
  grafana-data:
