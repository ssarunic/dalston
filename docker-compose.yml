# Dalston - Modular Audio Transcription Server
#
# Container Naming Convention:
#   {type}-{domain}-{stage}-{impl}[-v{version}][-cpu]
#   - type: batch (file processing) or rt (realtime streaming)
#   - domain: stt (speech-to-text), tts (text-to-speech, future)
#   - stage: prepare, transcribe, align, diarize, pii_detect, audio_redact, refine, merge
#   - impl: whisper, parakeet, pyannote, presidio, etc.
#   - version: v31, v40, etc. (only when multiple versions coexist)
#   - cpu: only when forcing CPU mode (GPU is default for compute-heavy engines)
#
# Image Sharing Strategy:
#   Services that use the same Dockerfile share a single image via the `image:` directive.
#   Only the first service has a `build:` section; others just reference the image.
#   This saves disk space and ensures consistency.
#
# Usage:
#   docker compose --profile local-infra --profile local-object-storage up -d
#   docker compose --profile local-infra --profile local-object-storage up -d --scale stt-batch-transcribe-faster-whisper-base=2

# ============================================================
# EXTENSION FIELDS (shared configurations)
# ============================================================

# Common environment variables for all engines
x-common-env: &common-env
  REDIS_URL: redis://redis:6379
  DALSTON_S3_BUCKET: ${DALSTON_S3_BUCKET:-dalston-artifacts}
  DALSTON_S3_REGION: ${DALSTON_S3_REGION:-eu-west-2}
  DALSTON_S3_ENDPOINT_URL: http://minio:9000
  DALSTON_S3_PUBLIC_ENDPOINT_URL: ${DALSTON_S3_PUBLIC_ENDPOINT_URL:-}
  AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minioadmin}
  AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
  DALSTON_LOG_LEVEL: ${DALSTON_LOG_LEVEL:-INFO}
  DALSTON_LOG_FORMAT: ${DALSTON_LOG_FORMAT:-json}

# Observability environment variables
x-observability-env: &observability-env
  OTEL_ENABLED: ${OTEL_ENABLED:-false}
  OTEL_EXPORTER_OTLP_ENDPOINT: ${OTEL_EXPORTER_OTLP_ENDPOINT:-http://jaeger:4317}
  DALSTON_METRICS_ENABLED: ${DALSTON_METRICS_ENABLED:-true}
  DALSTON_METRICS_PORT: 9100

# Core services environment (includes database)
x-core-env: &core-env
  <<: *common-env
  DATABASE_URL: postgresql+asyncpg://dalston:${POSTGRES_PASSWORD:-password}@postgres:5432/dalston
  DALSTON_ENGINE_UNAVAILABLE_BEHAVIOR: ${DALSTON_ENGINE_UNAVAILABLE_BEHAVIOR:-wait}
  DALSTON_ENGINE_WAIT_TIMEOUT_SECONDS: ${DALSTON_ENGINE_WAIT_TIMEOUT_SECONDS:-300}

# Dependencies for batch engines
x-batch-depends: &batch-depends
  redis:
    condition: service_healthy
    required: false
  minio-init:
    condition: service_completed_successfully
    required: false

# Dependencies for realtime engines
x-realtime-depends: &realtime-depends
  redis:
    condition: service_healthy
    required: false

# Dependencies for core services
x-core-depends: &core-depends
  postgres:
    condition: service_healthy
    required: false
  redis:
    condition: service_healthy
    required: false
  minio-init:
    condition: service_completed_successfully
    required: false

# GPU deployment configuration
x-gpu-deploy: &gpu-deploy
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]

# WebSocket healthcheck for realtime engines
x-ws-healthcheck: &ws-healthcheck
  test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.settimeout(2); s.connect(('localhost', 9000)); s.close()"]
  interval: 10s
  timeout: 5s
  retries: 3

services:
  # ============================================================
  # INFRASTRUCTURE
  # ============================================================

  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: dalston
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
      POSTGRES_DB: dalston
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dalston"]
      interval: 5s
      timeout: 5s
      retries: 5
    profiles: [local-infra]
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    # AOF persistence for durable Streams (M33)
    # - appendonly yes: enables append-only file persistence
    # - appendfsync everysec: fsync every second (balance of durability and performance)
    # - auto-aof-rewrite-percentage 100: rewrite AOF when it doubles in size
    # - auto-aof-rewrite-min-size 64mb: minimum size before rewrite
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --auto-aof-rewrite-percentage 100
      --auto-aof-rewrite-min-size 64mb
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    profiles: [local-infra]
    restart: unless-stopped

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 5s
      timeout: 5s
      retries: 5
    profiles: [local-object-storage]
    restart: unless-stopped

  minio-init:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set dalston http://minio:9000 $${MINIO_ROOT_USER:-minioadmin} $${MINIO_ROOT_PASSWORD:-minioadmin};
      mc mb dalston/$${DALSTON_S3_BUCKET:-dalston-artifacts} --ignore-existing;
      exit 0;
      "
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
      DALSTON_S3_BUCKET: ${DALSTON_S3_BUCKET:-dalston-artifacts}
    profiles: [local-object-storage]

  # Jaeger for distributed tracing (M19)
  jaeger:
    image: jaegertracing/all-in-one:1.54
    ports:
      - "16686:16686"
      - "4317:4317"
      - "4318:4318"
    environment:
      COLLECTOR_OTLP_ENABLED: "true"
    profiles: [observability]
    restart: unless-stopped

  # Prometheus for metrics collection (M20)
  prometheus:
    image: prom/prometheus:v2.50.0
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=1d'
      - '--storage.tsdb.retention.size=500MB'
    profiles: [observability]
    restart: unless-stopped

  # Grafana for dashboards (M20)
  grafana:
    image: grafana/grafana:10.3.0
    volumes:
      - ./docker/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: dalston
      GF_AUTH_ANONYMOUS_ENABLED: "true"
      GF_AUTH_ANONYMOUS_ORG_ROLE: Viewer
      GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH: /var/lib/grafana/dashboards/dalston-overview.json
    depends_on: [prometheus]
    profiles: [observability]
    restart: unless-stopped

  # Metrics exporter for Redis queue depths (M20)
  metrics-exporter:
    image: dalston/gateway:latest
    command: python -m dalston.metrics_exporter
    environment:
      <<: *common-env
      DALSTON_SCRAPE_INTERVAL: 15
    depends_on: *realtime-depends
    profiles: [observability]
    restart: unless-stopped

  # ============================================================
  # CORE SERVICES
  # ============================================================

  gateway:
    image: dalston/gateway:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.gateway
    ports:
      - "8000:8000"
    environment:
      <<: [*core-env, *observability-env]
    depends_on: *core-depends
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  orchestrator:
    image: dalston/orchestrator:latest
    build:
      context: .
      dockerfile: docker/Dockerfile.orchestrator
    environment:
      <<: [*core-env, *observability-env]
      DALSTON_METRICS_PORT: 8001
    depends_on: *core-depends
    restart: unless-stopped

  # ============================================================
  # BATCH STT ENGINES
  # ============================================================

  # --- PREPARE STAGE ---
  stt-batch-prepare:
    image: dalston/stt-batch-prepare:latest
    build:
      context: .
      dockerfile: engines/stt-prepare/audio-prepare/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: audio-prepare
    depends_on: *batch-depends
    restart: unless-stopped

  # --- TRANSCRIBE STAGE ---
  # M36: Runtime-based transcription engines. Each runtime can load any compatible
  # model variant on demand via config["runtime_model_id"].

  # NeMo Parakeet Runtime (GPU - English transcription)
  # Supports: nvidia/parakeet-ctc-0.6b, nvidia/parakeet-ctc-1.1b,
  #           nvidia/parakeet-tdt-0.6b-v3, nvidia/parakeet-tdt-1.1b
  stt-batch-transcribe-nemo:
    image: dalston/stt-batch-transcribe-nemo:1.0.0
    build:
      context: .
      dockerfile: engines/stt-transcribe/parakeet/Dockerfile
      args:
        DEVICE: cuda
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: nemo
      DALSTON_DEFAULT_MODEL_ID: "nvidia/parakeet-tdt-1.1b"
    volumes:
      - model-cache:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # Faster Whisper Runtime (CPU/GPU - multilingual transcription)
  # Supports: tiny, base, small, medium, large-v2, large-v3, large-v3-turbo
  # Default model (large-v3-turbo) is CPU-capable for out-of-the-box experience.
  stt-batch-transcribe-faster-whisper:
    image: dalston/stt-batch-transcribe-faster-whisper:1.0.0
    build:
      context: .
      dockerfile: engines/stt-transcribe/faster-whisper/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: faster-whisper
      DALSTON_DEFAULT_MODEL_ID: "large-v3-turbo"
    volumes:
      - model-cache:/models
    depends_on: *batch-depends
    restart: unless-stopped

  # Voxtral Mini 3B (GPU - multilingual transcription)
  # Note: Voxtral uses different framework (HF transformers), not part of M36 consolidation.
  stt-batch-transcribe-voxtral-mini-3b:
    image: dalston/stt-batch-transcribe-voxtral-mini-3b:1.0.0
    build:
      context: .
      dockerfile: engines/stt-transcribe/voxtral/Dockerfile
      args:
        MODEL_VARIANT: mini-3b
        DEVICE: cuda
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: voxtral-mini-3b
    volumes:
      - voxtral-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # --- ALIGN STAGE ---

  # Phoneme forced alignment (CPU - default for local dev)
  stt-batch-align-phoneme-cpu:
    image: dalston/stt-batch-align-phoneme:latest
    build:
      context: .
      dockerfile: engines/stt-align/phoneme-align/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: phoneme-align
    volumes:
      - align-models:/models
    depends_on: *batch-depends
    restart: unless-stopped

  # Phoneme forced alignment (GPU)
  stt-batch-align-phoneme:
    image: dalston/stt-batch-align-phoneme:latest
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: phoneme-align
    volumes:
      - align-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # --- DIARIZE STAGE ---

  # Pyannote 3.1 (CPU - default for local dev)
  stt-batch-diarize-pyannote-3.1-cpu:
    image: dalston/stt-batch-diarize-pyannote-3.1:latest
    build:
      context: .
      dockerfile: engines/stt-diarize/pyannote-3.1/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: pyannote-3.1
      HF_TOKEN: ${HF_TOKEN:-}
      DALSTON_DIARIZATION_DISABLED: ${DALSTON_DIARIZATION_DISABLED:-false}
    volumes:
      - pyannote-models:/models
    depends_on: *batch-depends
    restart: unless-stopped

  # Pyannote 3.1 (GPU)
  stt-batch-diarize-pyannote-3.1:
    image: dalston/stt-batch-diarize-pyannote-3.1:latest
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: pyannote-3.1
      HF_TOKEN: ${HF_TOKEN:-}
    volumes:
      - pyannote-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # Pyannote 4.0 (CPU - default for local dev)
  stt-batch-diarize-pyannote-4.0-cpu:
    image: dalston/stt-batch-diarize-pyannote-4.0:latest
    build:
      context: .
      dockerfile: engines/stt-diarize/pyannote-4.0/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: pyannote-4.0
      HF_TOKEN: ${HF_TOKEN:-}
      DALSTON_DIARIZATION_DISABLED: ${DALSTON_DIARIZATION_DISABLED:-false}
    volumes:
      - pyannote-models:/models
    depends_on: *batch-depends
    restart: unless-stopped

  # Pyannote 4.0 (GPU)
  stt-batch-diarize-pyannote-4.0:
    image: dalston/stt-batch-diarize-pyannote-4.0:latest
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: pyannote-4.0
      HF_TOKEN: ${HF_TOKEN:-}
    volumes:
      - pyannote-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # NeMo MSDD (GPU preferred, CPU allowed with NEMO_ALLOW_CPU=true)
  stt-batch-diarize-nemo-msdd:
    image: dalston/stt-batch-diarize-nemo-msdd:latest
    build:
      context: .
      dockerfile: engines/stt-diarize/nemo-msdd/Dockerfile
      args:
        SKIP_MODEL_DOWNLOAD: ${SKIP_MODEL_DOWNLOAD:-false}
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: nemo-msdd
      DALSTON_DIARIZATION_DISABLED: ${DALSTON_DIARIZATION_DISABLED:-false}
      DALSTON_NEMO_ALLOW_CPU: ${DALSTON_NEMO_ALLOW_CPU:-false}
    volumes:
      - nemo-models:/models
    depends_on: *batch-depends
    profiles: [gpu]
    restart: unless-stopped

  # --- PII DETECT STAGE ---
  stt-batch-pii-detect-presidio:
    image: dalston/stt-batch-pii-detect-presidio:latest
    build:
      context: .
      dockerfile: engines/stt-detect/pii-presidio/Dockerfile
      args:
        DEVICE: cpu
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: pii-presidio
      DALSTON_DEVICE: cpu
    volumes:
      - gliner-models:/models
    depends_on: *batch-depends
    restart: unless-stopped

  stt-batch-pii-detect-presidio-gpu:
    image: dalston/stt-batch-pii-detect-presidio-gpu:latest
    build:
      context: .
      dockerfile: engines/stt-detect/pii-presidio/Dockerfile
      args:
        DEVICE: cuda
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: pii-presidio
      DALSTON_DEVICE: cuda
    volumes:
      - gliner-models:/models
    depends_on: *batch-depends
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # --- AUDIO REDACT STAGE ---
  stt-batch-audio-redact-audio:
    image: dalston/stt-batch-audio-redact-audio:latest
    build:
      context: .
      dockerfile: engines/stt-redact/audio-redactor/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: audio-redactor
    depends_on: *batch-depends
    restart: unless-stopped

  # --- MERGE STAGE ---
  stt-batch-merge:
    image: dalston/stt-batch-merge:latest
    build:
      context: .
      dockerfile: engines/stt-merge/final-merger/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_ENGINE_ID: final-merger
    depends_on: *batch-depends
    restart: unless-stopped

  # ============================================================
  # REALTIME STT WORKERS
  # ============================================================

  # Parakeet RNNT 0.6B (GPU - default realtime worker, streaming capable)
  stt-rt-transcribe-parakeet-rnnt-0.6b:
    image: dalston/stt-rt-transcribe-parakeet-rnnt-0.6b:1.0.0
    build:
      context: .
      dockerfile: engines/stt-rt/parakeet/Dockerfile
      args:
        MODEL_VARIANT: 0.6b
        DEVICE: cuda
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_MODEL_VARIANT: 0.6b
      DALSTON_DEVICE: cuda
      DALSTON_WORKER_PORT: 9000
      DALSTON_MAX_SESSIONS: 4
    volumes:
      - model-cache:/models
    depends_on: *realtime-depends
    healthcheck: *ws-healthcheck
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # Parakeet RNNT 0.6B CPU (CPU-only realtime worker)
  stt-rt-transcribe-parakeet-rnnt-0.6b-cpu:
    image: dalston/stt-rt-transcribe-parakeet-rnnt-0.6b-cpu:1.0.0
    build:
      context: .
      dockerfile: engines/stt-rt/parakeet/Dockerfile
      args:
        MODEL_VARIANT: 0.6b
        DEVICE: cpu
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_MODEL_VARIANT: 0.6b
      DALSTON_DEVICE: cpu
      DALSTON_WORKER_PORT: 9000
      DALSTON_MAX_SESSIONS: 4
    volumes:
      - model-cache:/models
    depends_on: *realtime-depends
    healthcheck: *ws-healthcheck
    restart: unless-stopped

  # Parakeet RNNT 1.1B (GPU - accurate English streaming)
  stt-rt-transcribe-parakeet-rnnt-1.1b:
    image: dalston/stt-rt-transcribe-parakeet-rnnt-1.1b:1.0.0
    build:
      context: .
      dockerfile: engines/stt-rt/parakeet/Dockerfile
      args:
        MODEL_VARIANT: 1.1b
        DEVICE: cuda
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_MODEL_VARIANT: 1.1b
      DALSTON_WORKER_PORT: 9000
      DALSTON_MAX_SESSIONS: 4
    volumes:
      - model-cache:/models
    depends_on: *realtime-depends
    healthcheck: *ws-healthcheck
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # Faster Whisper base (GPU)
  stt-rt-transcribe-faster-whisper-base:
    image: dalston/stt-rt-transcribe-faster-whisper-base:1.0.0
    build:
      context: .
      dockerfile: engines/stt-rt/faster-whisper/Dockerfile
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_WORKER_PORT: 9000
      DALSTON_MAX_SESSIONS: 4
    volumes:
      - realtime-models:/models
    depends_on: *realtime-depends
    healthcheck: *ws-healthcheck
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

  # Voxtral Mini 4B (GPU - multilingual realtime)
  stt-rt-transcribe-voxtral-mini-4b:
    image: dalston/stt-rt-transcribe-voxtral-mini-4b:1.0.0
    build:
      context: .
      dockerfile: engines/stt-rt/voxtral/Dockerfile
      args:
        MODEL_VARIANT: mini-4b
    environment:
      <<: [*common-env, *observability-env]
      DALSTON_MODEL_VARIANT: mini-4b
      DALSTON_WORKER_PORT: 9000
      DALSTON_MAX_SESSIONS: 4
      DALSTON_TRANSCRIPTION_DELAY_MS: 480
    volumes:
      - voxtral-models:/models
    depends_on: *realtime-depends
    healthcheck: *ws-healthcheck
    deploy: *gpu-deploy
    profiles: [gpu]
    restart: unless-stopped

volumes:
  postgres-data:
  redis-data:
  minio-data:
  model-cache:        # M36: Shared model cache for all runtime engines
  align-models:
  pyannote-models:
  nemo-models:
  realtime-models:
  voxtral-models:
  gliner-models:
  prometheus-data:
  grafana-data:
