# M22 Task 22.1: Parakeet Batch Engine - Implementation Plan

## Overview

This plan covers the implementation of the Parakeet batch transcription engine at `engines/transcribe/parakeet/`. The engine uses NVIDIA NeMo's FastConformer RNNT model for high-throughput English transcription, producing word-level timestamps natively without a separate alignment stage.

The engine follows the same patterns established in `engines/transcribe/faster-whisper/` — it extends the `Engine` base class from `dalston/engine_sdk`, polls a Redis queue, downloads audio from S3, runs inference, and uploads structured results.

## Goal

Provide a batch transcription engine that:

- Transcribes English audio using Parakeet RNNT (0.6B or 1.1B)
- Produces segments with word-level timestamps from RNNT alignment
- Achieves RTFx >2000 on A10G-class GPUs
- Integrates with the existing batch pipeline without changes to downstream stages
- Signals to the orchestrator that ALIGN can be skipped (native word timestamps)

---

## Architecture

```
Task from Redis Queue (dalston:queue:parakeet)
    ↓
EngineRunner (from engine_sdk)
├─ Downloads audio from S3
├─ Loads task config
└─ Calls ParakeetEngine.process(input)
    ↓
ParakeetEngine
├─ Loads NeMo RNNT model (lazy, cached)
├─ Runs inference via EncDecRNNTBPEModel.transcribe()
├─ Extracts segments + word timestamps from hypothesis
└─ Returns TaskOutput with normalized data
    ↓
EngineRunner
├─ Uploads output to S3
├─ Publishes task.completed event
└─ Cleans up temp files
```

**Key difference from faster-whisper**: Parakeet uses NeMo's `EncDecRNNTBPEModel` instead of CTranslate2's `WhisperModel`. The NeMo API returns `Hypothesis` objects with timestamp information encoded differently — word boundaries come from RNNT alignment rather than cross-attention weights.

---

## File Structure

```
engines/transcribe/parakeet/
├── Dockerfile           # Multi-stage build with NeMo toolkit
├── requirements.txt     # NeMo, torch, torchaudio, soundfile
├── engine.yaml          # Capabilities, config schema, output schema
└── engine.py            # ParakeetEngine implementation
```

---

## Component Details

### 1. `engine.yaml` - Engine Manifest

```yaml
id: parakeet
stage: transcribe
name: NVIDIA Parakeet
version: 1.0.0
description: |
  NVIDIA Parakeet FastConformer RNNT for high-speed English transcription.
  Produces word-level timestamps natively via RNNT alignment.
  Requires NVIDIA GPU with CUDA support.

container:
  gpu: required
  memory: 8G
  model_cache: /models

capabilities:
  languages:
    - en
  max_audio_duration: 7200
  streaming: false
  word_timestamps: true
  native_word_timestamps: true    # Signals ALIGN stage can be skipped

input:
  audio_formats:
    - wav
  sample_rate: 16000
  channels: 1

config_schema:
  type: object
  properties:
    model:
      type: string
      enum:
        - nvidia/parakeet-rnnt-0.6b
        - nvidia/parakeet-rnnt-1.1b
        - nvidia/parakeet-ctc-0.6b
        - nvidia/parakeet-ctc-1.1b
      default: nvidia/parakeet-rnnt-0.6b
    batch_size:
      type: integer
      default: 1
      minimum: 1
      maximum: 32
      description: Batch size for long audio chunked inference
    compute_type:
      type: string
      enum: [float16, bfloat16, float32]
      default: float16
    return_hypotheses:
      type: boolean
      default: true
      description: Return full hypotheses with timestamps (should stay true)
  additionalProperties: false

output_schema:
  type: object
  required: [text, segments, language]
  properties:
    text:
      type: string
    segments:
      type: array
      items:
        type: object
        required: [start, end, text]
        properties:
          start:
            type: number
          end:
            type: number
          text:
            type: string
          words:
            type: array
            items:
              type: object
              properties:
                word: {type: string}
                start: {type: number}
                end: {type: number}
                confidence: {type: number}
    language:
      type: string
      description: Always "en" for Parakeet
    native_word_timestamps:
      type: boolean
      description: True — downstream ALIGN stage can be skipped
```

**Key additions vs faster-whisper:**

- `capabilities.native_word_timestamps: true` — Orchestrator reads this to skip the ALIGN stage
- `capabilities.languages` restricted to `["en"]`
- `container.gpu: required` (not optional — no CPU fallback for production)
- Config schema uses NeMo model identifiers instead of Whisper size names

---

### 2. `engine.py` - ParakeetEngine Implementation

```python
import logging
from pathlib import Path
from typing import Any

from dalston.engine_sdk import Engine, TaskInput, TaskOutput

logger = logging.getLogger(__name__)


class ParakeetEngine(Engine):
    """NVIDIA Parakeet FastConformer RNNT batch transcription engine.

    Uses NeMo's EncDecRNNTBPEModel for high-speed English transcription
    with native word-level timestamps from RNNT alignment.
    """

    DEFAULT_MODEL = "nvidia/parakeet-rnnt-0.6b"
    DEFAULT_COMPUTE_TYPE = "float16"

    def __init__(self) -> None:
        super().__init__()
        self._model = None
        self._model_name: str | None = None
        self._device: str = "cuda"

    def _detect_device(self) -> str:
        """Verify CUDA is available. Parakeet requires GPU."""
        try:
            import torch
            if torch.cuda.is_available():
                return "cuda"
        except ImportError:
            pass
        logger.warning("CUDA not available — Parakeet requires NVIDIA GPU")
        return "cpu"

    def _load_model(self, model_name: str) -> None:
        """Load NeMo RNNT model. Cached across tasks."""
        if self._model is not None and self._model_name == model_name:
            return

        import nemo.collections.asr as nemo_asr
        import torch

        self._device = self._detect_device()
        logger.info(f"Loading Parakeet model: {model_name} on {self._device}")

        if "rnnt" in model_name:
            self._model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(
                model_name=model_name
            )
        elif "ctc" in model_name:
            self._model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(
                model_name=model_name
            )
        else:
            raise ValueError(f"Unknown Parakeet model type: {model_name}")

        if self._device == "cuda":
            self._model = self._model.cuda()
            dtype = torch.float16
            self._model = self._model.to(dtype)

        self._model.eval()
        self._model_name = model_name
        logger.info(f"Parakeet model loaded: {model_name}")

    def process(self, input: TaskInput) -> TaskOutput:
        """Transcribe audio file using Parakeet RNNT."""
        config = input.config

        model_name = config.get("model", self.DEFAULT_MODEL)

        # Load model (lazy, cached)
        self._load_model(model_name)

        # Run transcription with timestamps
        output = self._model.transcribe(
            [str(input.audio_path)],
            return_hypotheses=True,
            batch_size=config.get("batch_size", 1),
        )

        # Extract hypothesis — NeMo returns list of Hypothesis objects
        # For RNNT models, timestamps are in hypothesis.timestep
        # For CTC models, timestamps are in hypothesis.timestep
        if isinstance(output, tuple):
            hypotheses = output[0]  # (hypotheses, _)
        else:
            hypotheses = output

        hypothesis = hypotheses[0]
        full_text = hypothesis.text

        # Extract word-level timestamps from hypothesis
        segments = self._extract_segments(hypothesis)

        return TaskOutput(
            data={
                "text": full_text,
                "segments": segments,
                "language": "en",
                "native_word_timestamps": True,
            }
        )

    def _extract_segments(self, hypothesis) -> list[dict[str, Any]]:
        """Extract segments with word timestamps from NeMo hypothesis.

        NeMo RNNT models provide word-level timing via the
        hypothesis.timestep attribute. We group words into segments
        based on punctuation and pause boundaries.
        """
        words = []

        if hasattr(hypothesis, "timestep") and hypothesis.timestep is not None:
            timesteps = hypothesis.timestep
            token_texts = hypothesis.text.split()

            # NeMo provides per-token timestamps — map to words
            for i, word_text in enumerate(token_texts):
                if i < len(timesteps.get("word", [])):
                    word_ts = timesteps["word"][i]
                    words.append({
                        "word": word_text,
                        "start": round(word_ts["start"], 3),
                        "end": round(word_ts["end"], 3),
                        "confidence": round(
                            word_ts.get("score", 0.95), 3
                        ),
                    })
                else:
                    words.append({"word": word_text})

        elif hasattr(hypothesis, "word_timestamps"):
            # Alternative attribute name in some NeMo versions
            for wt in hypothesis.word_timestamps:
                words.append({
                    "word": wt.word,
                    "start": round(wt.start, 3),
                    "end": round(wt.end, 3),
                    "confidence": round(getattr(wt, "score", 0.95), 3),
                })

        # Group words into segments by sentence boundaries
        segments = []
        current_words = []
        sentence_endings = {".", "!", "?"}

        for word in words:
            current_words.append(word)
            if word.get("word", "").rstrip()[-1:] in sentence_endings:
                seg_text = " ".join(w["word"] for w in current_words)
                segment = {
                    "start": current_words[0].get("start", 0.0),
                    "end": current_words[-1].get("end", 0.0),
                    "text": seg_text,
                    "words": current_words,
                }
                segments.append(segment)
                current_words = []

        # Remaining words form a final segment
        if current_words:
            seg_text = " ".join(w["word"] for w in current_words)
            segment = {
                "start": current_words[0].get("start", 0.0),
                "end": current_words[-1].get("end", 0.0),
                "text": seg_text,
                "words": current_words,
            }
            segments.append(segment)

        # Fallback: if no word timestamps, create a single segment
        if not segments and hypothesis.text:
            segments = [{
                "start": 0.0,
                "end": 0.0,
                "text": hypothesis.text,
            }]

        return segments

    def health_check(self) -> dict[str, Any]:
        """Report health status and GPU info."""
        try:
            import torch
            cuda_available = torch.cuda.is_available()
            cuda_device_count = torch.cuda.device_count() if cuda_available else 0
            gpu_memory_used = (
                f"{torch.cuda.memory_allocated() / 1e9:.1f}GB"
                if cuda_available else "0GB"
            )
        except ImportError:
            cuda_available = False
            cuda_device_count = 0
            gpu_memory_used = "0GB"

        return {
            "status": "healthy",
            "model_loaded": self._model is not None,
            "model_name": self._model_name,
            "device": self._device,
            "cuda_available": cuda_available,
            "cuda_device_count": cuda_device_count,
            "gpu_memory_used": gpu_memory_used,
        }


if __name__ == "__main__":
    engine = ParakeetEngine()
    engine.run()
```

**Key differences from FasterWhisperEngine:**

1. **NeMo API** — Uses `EncDecRNNTBPEModel.from_pretrained()` and `.transcribe()` instead of CTranslate2
2. **Hypothesis extraction** — NeMo returns `Hypothesis` objects with `timestep` dict instead of segment generators
3. **Word timestamps** — Extracted from RNNT alignment (`hypothesis.timestep`) rather than cross-attention weights
4. **Segment grouping** — Groups words into segments by sentence boundaries since NeMo returns a flat word list
5. **GPU required** — Logs a warning and degrades on CPU (NeMo can technically run on CPU but too slow for production)
6. **Language** — Always returns `"en"` since Parakeet is English-only
7. **`native_word_timestamps: True`** — Signals to orchestrator that ALIGN stage can be skipped

---

### 3. `requirements.txt`

```
nemo_toolkit[asr]>=2.0.0,<3.0.0
torch>=2.1.0
torchaudio>=2.1.0
soundfile>=0.12.0
```

**Notes:**

- `nemo_toolkit[asr]` pulls in NeMo's ASR collection including model classes and pre-trained model download utilities
- Torch version must be compatible with NeMo and the target CUDA version
- `soundfile` is required by NeMo for audio file loading
- Pin NeMo major version to avoid breaking API changes

---

### 4. `Dockerfile`

```dockerfile
# Stage 1: Install dependencies
FROM python:3.11-slim AS builder

RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    ffmpeg \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /opt/dalston

# Install dalston engine SDK
COPY pyproject.toml .
COPY dalston/ dalston/
RUN pip install --no-cache-dir -e ".[engine-sdk]"

WORKDIR /engine

# Install engine requirements
COPY engines/transcribe/parakeet/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Pre-download default model at build time
RUN python -c "\
import nemo.collections.asr as nemo_asr; \
model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained('nvidia/parakeet-rnnt-0.6b'); \
print(f'Model downloaded: {model.__class__.__name__}')"

# Stage 2: Runtime image
FROM python:3.11-slim

RUN apt-get update && apt-get install -y --no-install-recommends \
    ffmpeg \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin
COPY --from=builder /opt/dalston /opt/dalston

WORKDIR /engine

COPY engines/transcribe/parakeet/engine.yaml .
COPY engines/transcribe/parakeet/engine.py .

# Model cache volume
ENV HF_HOME=/models
ENV NEMO_CACHE_DIR=/models/nemo
RUN mkdir -p /models/nemo

# Copy pre-downloaded model from builder
COPY --from=builder /root/.cache/huggingface /models
COPY --from=builder /root/.cache/torch/NeMo /models/nemo

CMD ["python", "engine.py"]
```

**Build notes:**

- Multi-stage build keeps the runtime image smaller by excluding build tools
- Model is pre-downloaded during build to avoid startup delays
- `NEMO_CACHE_DIR` ensures NeMo uses the volume-mounted cache
- `libsndfile1` is required by soundfile for audio I/O
- The image will be large (~4-6GB) due to NeMo + PyTorch + model weights

---

## Orchestrator Changes

The orchestrator needs a small update to skip the ALIGN stage when using an engine that provides native word timestamps.

**In `dalston/orchestrator/dag.py`:**

When building the task DAG, check the engine's `native_word_timestamps` capability:

```python
# When building DAG for a job
engine_id = model_definition.engine  # "parakeet" or "faster-whisper"

# Look up engine capabilities (from engine.yaml or registry)
if engine_has_native_word_timestamps(engine_id):
    # Skip ALIGN stage — go directly from TRANSCRIBE to DIARIZE
    # TRANSCRIBE → DIARIZE → MERGE
    pass
else:
    # Standard pipeline: TRANSCRIBE → ALIGN → DIARIZE → MERGE
    pass
```

This is a conditional optimization, not a hard requirement. If ALIGN is included anyway, it should detect that word timestamps already exist and pass through without modification.

---

## Implementation Order

**Phase 1: Engine Core**

1. Create `engines/transcribe/parakeet/` directory
2. Write `engine.yaml` with capabilities and schemas
3. Write `requirements.txt` with NeMo dependencies
4. Implement `ParakeetEngine` in `engine.py` — model loading and inference
5. Implement `_extract_segments()` — word timestamp extraction and segment grouping

**Phase 2: Container**

6. Write `Dockerfile` with multi-stage build
7. Test local build: `docker build -t dalston-parakeet engines/transcribe/parakeet/`
8. Verify model pre-download completes in build

**Phase 3: Integration**

9. Add Docker Compose service definition for `stt-batch-transcribe-parakeet`
10. Update orchestrator DAG to skip ALIGN when `native_word_timestamps` is true
11. Test end-to-end: submit job with `model=parakeet-0.6b`, verify transcription

**Phase 4: Testing**

12. Write unit tests with mocked NeMo model
13. Write integration test for full batch flow
14. Verify output format matches faster-whisper's segment/word structure

---

## Verification Checklist

After implementation, verify:

- [ ] `ParakeetEngine` loads `nvidia/parakeet-rnnt-0.6b` successfully on GPU
- [ ] Transcription produces text with word-level timestamps
- [ ] Output format (segments, words) matches faster-whisper's schema
- [ ] `native_word_timestamps: true` is included in output data
- [ ] Orchestrator skips ALIGN stage when Parakeet is selected
- [ ] Docker build completes with pre-downloaded model
- [ ] Engine starts and polls `dalston:queue:parakeet` queue
- [ ] Health check reports GPU status and model info
- [ ] Switching between 0.6B and 1.1B models works via task config
- [ ] Error handling: clear message when CUDA is unavailable

---

## Design Decisions

1. **RNNT over CTC** — RNNT is the default because it provides better word-level timestamps through autoregressive alignment. CTC variants are available as config options for users who prefer faster decoding at the cost of slightly less accurate timing.

2. **Sentence-based segmentation** — Parakeet returns a flat word list with timestamps, unlike Whisper which returns pre-segmented output. We group words into segments at sentence boundaries (`.`, `!`, `?`) to maintain compatibility with the existing pipeline. An alternative would be pause-based segmentation using timestamp gaps.

3. **GPU required** — Unlike faster-whisper which gracefully falls back to CPU with `int8` quantization, Parakeet's NeMo implementation is impractical on CPU. The engine logs a warning but does not refuse to start on CPU, allowing development/testing without GPU.

4. **Multi-stage Docker build** — NeMo's build dependencies are heavy. The multi-stage build reduces the runtime image size while keeping build-time tools in the builder stage.

5. **Model pre-download** — The default 0.6B model is downloaded at Docker build time. The 1.1B model is downloaded on first use (lazy) since including both in the image would significantly increase size. Users who need 1.1B can extend the Dockerfile or mount a model cache volume.

---

## Notes

- NeMo's `transcribe()` API may change between versions. Pin `nemo_toolkit>=2.0.0,<3.0.0` and test before upgrading.
- The `hypothesis.timestep` format varies between NeMo versions. The implementation includes fallback handling for `word_timestamps` attribute.
- For very long audio (>1 hour), NeMo's internal chunking handles segmentation. The `batch_size` config controls chunk parallelism.
- Parakeet models include built-in punctuation and capitalization, so no post-processing is needed (unlike some Whisper models that produce lowercase output).
