# M14 Task 14.1: Model Selection - Implementation Plan

## Overview

This plan covers the implementation of model selection for the transcription API, allowing users to choose between different Whisper model sizes and future engines (Parakeet, Canary-Qwen).

**Specification:** [MODEL_SELECTION.md](/docs/specs/MODEL_SELECTION.md)

## Goal

Enable users to select transcription models via a single `model` parameter, with:
- Provider-style model names (`whisper-large-v3`, `whisper-base`, `distil-whisper`)
- Automatic engine routing based on model
- Convenience aliases (`fast`, `accurate`)
- Model discovery via `/v1/models` endpoint

---

## Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           MODEL SELECTION FLOW                               │
│                                                                              │
│   User Request                                                               │
│   model="whisper-base"                                                       │
│         │                                                                    │
│         ▼                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                        Gateway                                       │   │
│   │                                                                      │   │
│   │   1. Resolve alias (if any)                                         │   │
│   │   2. Lookup in MODEL_REGISTRY                                       │   │
│   │   3. Validate model exists                                          │   │
│   │   4. Extract engine + engine_model                                  │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│         │                                                                    │
│         │  engine="faster-whisper"                                          │
│         │  engine_model="base"                                              │
│         ▼                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                      Orchestrator                                    │   │
│   │                                                                      │   │
│   │   1. Build DAG with engine routing                                  │   │
│   │   2. Pass engine_model in task config                               │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│         │                                                                    │
│         │  config.model="base"                                              │
│         ▼                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                    faster-whisper Engine                             │   │
│   │                                                                      │   │
│   │   Loads "base" model (already supports this!)                       │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## File Structure

```
dalston/common/
├── __init__.py           # Package init
└── models.py             # Model registry, resolution, types

dalston/gateway/api/v1/
├── models.py             # GET /v1/models endpoint (new)
├── transcription.py      # Add model parameter (modify)
└── realtime.py           # Accept full model IDs (modify)

dalston/orchestrator/
└── dag.py                # Use model for engine routing (modify)

tests/
├── unit/test_models.py   # Registry unit tests (new)
└── integration/test_model_selection.py  # API tests (new)
```

---

## Component Details

### 1. `dalston/common/models.py` - Model Registry

```python
from dataclasses import dataclass
from typing import Literal

@dataclass
class ModelDefinition:
    """Definition of a transcription model."""
    id: str                    # API-facing identifier
    engine: str                # Engine ID (matches engine.yaml)
    engine_model: str          # Model name passed to engine
    name: str                  # Human-readable name
    description: str           # Brief description
    tier: Literal["fast", "balanced", "accurate"]
    languages: int             # Number of supported languages
    streaming: bool            # Supports real-time streaming
    word_timestamps: bool      # Supports word-level timing
    vram_gb: float             # Approximate VRAM requirement
    speed_factor: float        # Relative speed (1.0 = baseline)


MODEL_REGISTRY: dict[str, ModelDefinition] = {
    "whisper-large-v3": ModelDefinition(
        id="whisper-large-v3",
        engine="faster-whisper",
        engine_model="large-v3",
        name="Whisper Large v3",
        description="Most accurate multilingual model, 99 languages",
        tier="accurate",
        languages=99,
        streaming=False,
        word_timestamps=True,
        vram_gb=10.0,
        speed_factor=1.0,
    ),
    "whisper-large-v2": ModelDefinition(
        id="whisper-large-v2",
        engine="faster-whisper",
        engine_model="large-v2",
        name="Whisper Large v2",
        description="Previous generation large model",
        tier="accurate",
        languages=99,
        streaming=False,
        word_timestamps=True,
        vram_gb=10.0,
        speed_factor=1.0,
    ),
    "whisper-medium": ModelDefinition(
        id="whisper-medium",
        engine="faster-whisper",
        engine_model="medium",
        name="Whisper Medium",
        description="Balanced accuracy and speed",
        tier="balanced",
        languages=99,
        streaming=False,
        word_timestamps=True,
        vram_gb=5.0,
        speed_factor=2.0,
    ),
    "whisper-small": ModelDefinition(
        id="whisper-small",
        engine="faster-whisper",
        engine_model="small",
        name="Whisper Small",
        description="Fast multilingual transcription",
        tier="fast",
        languages=99,
        streaming=False,
        word_timestamps=True,
        vram_gb=2.0,
        speed_factor=4.0,
    ),
    "whisper-base": ModelDefinition(
        id="whisper-base",
        engine="faster-whisper",
        engine_model="base",
        name="Whisper Base",
        description="Very fast, lower accuracy",
        tier="fast",
        languages=99,
        streaming=False,
        word_timestamps=True,
        vram_gb=1.0,
        speed_factor=8.0,
    ),
    "whisper-tiny": ModelDefinition(
        id="whisper-tiny",
        engine="faster-whisper",
        engine_model="tiny",
        name="Whisper Tiny",
        description="Fastest, minimal accuracy",
        tier="fast",
        languages=99,
        streaming=False,
        word_timestamps=True,
        vram_gb=0.5,
        speed_factor=16.0,
    ),
    "distil-whisper": ModelDefinition(
        id="distil-whisper",
        engine="faster-whisper",
        engine_model="distil-large-v3",
        name="Distil-Whisper",
        description="Fast English-only, near large-v3 accuracy",
        tier="fast",
        languages=1,
        streaming=False,
        word_timestamps=True,
        vram_gb=5.0,
        speed_factor=6.0,
    ),
}

MODEL_ALIASES: dict[str, str] = {
    "fast": "distil-whisper",
    "accurate": "whisper-large-v3",
    "large": "whisper-large-v3",
    "medium": "whisper-medium",
    "small": "whisper-small",
    "base": "whisper-base",
    "tiny": "whisper-tiny",
}

DEFAULT_MODEL = "whisper-large-v3"


def resolve_model(model_id: str) -> ModelDefinition:
    """Resolve model ID or alias to ModelDefinition.

    Args:
        model_id: Model identifier or alias

    Returns:
        ModelDefinition for the resolved model

    Raises:
        ValueError: If model not found
    """
    resolved_id = MODEL_ALIASES.get(model_id, model_id)
    if resolved_id not in MODEL_REGISTRY:
        available = ", ".join(sorted(MODEL_REGISTRY.keys()))
        raise ValueError(
            f"Unknown model: '{model_id}'. Available models: {available}"
        )
    return MODEL_REGISTRY[resolved_id]


def get_available_models() -> list[ModelDefinition]:
    """Return all registered models."""
    return list(MODEL_REGISTRY.values())


def get_model_ids() -> list[str]:
    """Return all model IDs (without aliases)."""
    return list(MODEL_REGISTRY.keys())
```

---

### 2. `dalston/gateway/api/v1/transcription.py` - Add Model Parameter

Changes to existing file:

```python
# Add import
from dalston.common.models import resolve_model, DEFAULT_MODEL

# In TranscriptionCreateParams (or form parameters)
model: str = Form(
    default=DEFAULT_MODEL,
    description="Transcription model to use (e.g., whisper-large-v3, whisper-base, fast)"
)

# In create_transcription endpoint, before job creation
try:
    model_def = resolve_model(params.model)
except ValueError as e:
    raise HTTPException(
        status_code=400,
        detail={
            "type": "invalid_request_error",
            "message": str(e),
            "param": "model"
        }
    )

# Pass to job creation
job = await create_job(
    file_path=...,
    params={
        "model": model_def.id,
        "engine_transcribe": model_def.engine,
        "transcribe_config": {
            "model": model_def.engine_model,
            # ... other config
        },
        # ... other params
    }
)
```

---

### 3. `dalston/gateway/api/v1/models.py` - Models Endpoint (New)

```python
from fastapi import APIRouter, HTTPException

from dalston.common.models import (
    get_available_models,
    resolve_model,
    MODEL_ALIASES,
)

router = APIRouter(prefix="/v1/models", tags=["models"])


@router.get("")
async def list_models():
    """List available transcription models.

    Returns all models that can be used with the `model` parameter
    in transcription requests.
    """
    models = get_available_models()
    return {
        "object": "list",
        "data": [
            {
                "id": m.id,
                "object": "model",
                "name": m.name,
                "description": m.description,
                "capabilities": {
                    "languages": m.languages,
                    "streaming": m.streaming,
                    "word_timestamps": m.word_timestamps,
                },
                "tier": m.tier,
            }
            for m in models
        ],
        "aliases": MODEL_ALIASES,
    }


@router.get("/{model_id}")
async def get_model(model_id: str):
    """Get details for a specific model.

    Args:
        model_id: Model identifier or alias
    """
    try:
        model = resolve_model(model_id)
    except ValueError:
        raise HTTPException(
            status_code=404,
            detail=f"Model not found: {model_id}"
        )

    return {
        "id": model.id,
        "object": "model",
        "name": model.name,
        "description": model.description,
        "capabilities": {
            "languages": model.languages,
            "streaming": model.streaming,
            "word_timestamps": model.word_timestamps,
        },
        "tier": model.tier,
        "engine": model.engine,
        "resource_hints": {
            "vram_gb": model.vram_gb,
            "speed_factor": model.speed_factor,
        },
    }
```

**Register in `dalston/gateway/main.py`:**

```python
from dalston.gateway.api.v1 import models

app.include_router(models.router)
```

---

### 4. `dalston/orchestrator/dag.py` - Engine Routing

Changes to DAG builder:

```python
from dalston.common.models import resolve_model, DEFAULT_MODEL

def build_dag(self, job: Job) -> list[Task]:
    # Get model from job params
    model_id = job.params.get("model", DEFAULT_MODEL)

    try:
        model_def = resolve_model(model_id)
    except ValueError:
        # Fallback to default if invalid (shouldn't happen if gateway validates)
        model_def = resolve_model(DEFAULT_MODEL)

    # Use model's engine for transcription task
    engine_transcribe = job.params.get("engine_transcribe", model_def.engine)

    # Build transcribe config with model's engine_model
    transcribe_config = {
        **DEFAULT_TRANSCRIBE_CONFIG,
        "model": model_def.engine_model,
        **job.params.get("transcribe_config", {}),
    }

    # ... rest of DAG building
```

---

### 5. `dalston/gateway/api/v1/realtime.py` - WebSocket Updates

```python
from dalston.common.models import resolve_model

@router.websocket("/stream")
async def realtime_transcription(
    websocket: WebSocket,
    model: str = Query(default="fast"),
    # ... other params
):
    # Resolve model (handles aliases)
    try:
        model_def = resolve_model(model)
    except ValueError as e:
        await websocket.close(code=4000, reason=str(e))
        return

    # Check streaming support
    if not model_def.streaming:
        # For non-streaming models, use the realtime-capable equivalent
        # or return error if no streaming equivalent exists
        pass

    # Pass to session router
    # ...
```

---

## Testing

### Unit Tests (`tests/unit/test_models.py`)

```python
import pytest
from dalston.common.models import (
    resolve_model,
    get_available_models,
    MODEL_REGISTRY,
    MODEL_ALIASES,
    DEFAULT_MODEL,
)


class TestResolveModel:
    def test_direct_model_id(self):
        model = resolve_model("whisper-large-v3")
        assert model.id == "whisper-large-v3"
        assert model.engine == "faster-whisper"
        assert model.engine_model == "large-v3"

    def test_alias_resolution(self):
        model = resolve_model("fast")
        assert model.id == "distil-whisper"

    def test_accurate_alias(self):
        model = resolve_model("accurate")
        assert model.id == "whisper-large-v3"

    def test_unknown_model_raises(self):
        with pytest.raises(ValueError) as exc:
            resolve_model("nonexistent-model")
        assert "Unknown model" in str(exc.value)
        assert "whisper-large-v3" in str(exc.value)  # Shows available

    def test_all_aliases_resolve(self):
        for alias, expected_id in MODEL_ALIASES.items():
            model = resolve_model(alias)
            assert model.id == expected_id


class TestGetAvailableModels:
    def test_returns_all_models(self):
        models = get_available_models()
        assert len(models) == len(MODEL_REGISTRY)

    def test_default_model_exists(self):
        models = get_available_models()
        ids = [m.id for m in models]
        assert DEFAULT_MODEL in ids


class TestModelDefinitions:
    def test_all_models_have_required_fields(self):
        for model in get_available_models():
            assert model.id
            assert model.engine
            assert model.engine_model
            assert model.name
            assert model.tier in ("fast", "balanced", "accurate")
            assert model.languages > 0
```

### Integration Tests (`tests/integration/test_model_selection.py`)

```python
import pytest
import httpx

BASE_URL = "http://localhost:8000"


@pytest.mark.asyncio
async def test_list_models():
    async with httpx.AsyncClient() as client:
        response = await client.get(f"{BASE_URL}/v1/models")
        assert response.status_code == 200
        data = response.json()
        assert "data" in data
        assert len(data["data"]) > 0
        # Check structure
        model = data["data"][0]
        assert "id" in model
        assert "capabilities" in model


@pytest.mark.asyncio
async def test_get_model_by_id():
    async with httpx.AsyncClient() as client:
        response = await client.get(f"{BASE_URL}/v1/models/whisper-large-v3")
        assert response.status_code == 200
        data = response.json()
        assert data["id"] == "whisper-large-v3"
        assert "engine" in data


@pytest.mark.asyncio
async def test_get_model_by_alias():
    async with httpx.AsyncClient() as client:
        response = await client.get(f"{BASE_URL}/v1/models/fast")
        assert response.status_code == 200
        data = response.json()
        assert data["id"] == "distil-whisper"


@pytest.mark.asyncio
async def test_get_unknown_model():
    async with httpx.AsyncClient() as client:
        response = await client.get(f"{BASE_URL}/v1/models/nonexistent")
        assert response.status_code == 404


@pytest.mark.asyncio
async def test_transcription_with_model(audio_file):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{BASE_URL}/v1/audio/transcriptions",
            files={"file": audio_file},
            data={"model": "whisper-base"},
        )
        assert response.status_code in (200, 201, 202)


@pytest.mark.asyncio
async def test_transcription_with_invalid_model(audio_file):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{BASE_URL}/v1/audio/transcriptions",
            files={"file": audio_file},
            data={"model": "invalid-model"},
        )
        assert response.status_code == 400
        data = response.json()
        assert "model" in str(data)
```

---

## File Change Summary

| File | Action | Description |
|------|--------|-------------|
| `dalston/common/__init__.py` | Create | Package init |
| `dalston/common/models.py` | Create | Model registry and resolution |
| `dalston/gateway/api/v1/transcription.py` | Modify | Add model parameter |
| `dalston/gateway/api/v1/models.py` | Create | Models list endpoint |
| `dalston/gateway/api/v1/realtime.py` | Modify | Support full model IDs |
| `dalston/gateway/main.py` | Modify | Register models router |
| `dalston/orchestrator/dag.py` | Modify | Use model for engine selection |
| `dalston/session_router/registry.py` | Modify | Route by model's engine |
| `tests/unit/test_models.py` | Create | Unit tests |
| `tests/integration/test_model_selection.py` | Create | Integration tests |

---

## Implementation Order

1. **Create model registry** (`dalston/common/models.py`) - foundation
2. **Add models endpoint** (`dalston/gateway/api/v1/models.py`) - allows testing
3. **Add model param to batch API** - core functionality
4. **Update orchestrator** - connect API to engines
5. **Update real-time API** - consistency
6. **Add tests** - validation
7. **Update docs** - document new parameter

---

## Rollout

1. Deploy with `whisper-large-v3` as default (no behavior change for existing users)
2. Document new `model` parameter in API docs
3. Monitor for validation errors in logs
4. Add more models to registry as new engines are implemented
