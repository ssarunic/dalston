# M22 Task 22.2: Parakeet Real-time Engine - Implementation Plan

## Overview

This plan covers the implementation of the Parakeet real-time streaming transcription engine at `engines/realtime/parakeet-streaming/`. The engine uses NVIDIA NeMo's cache-aware streaming inference to deliver sub-100ms latency transcription over WebSocket connections.

Unlike the existing whisper-streaming engine which uses chunked re-encoding (buffer audio → transcribe chunk → diff against previous), Parakeet's FastConformer encoder natively supports streaming through a cache-aware architecture. The encoder maintains internal state across chunks, processing each new chunk incrementally without re-processing previous audio.

This engine extends `RealtimeEngine` from `dalston/realtime_sdk` and follows the patterns established in `engines/realtime/whisper-streaming/`.

## Goal

Provide a real-time transcription engine that:

- Streams transcription results with sub-100ms latency using cache-aware inference
- Processes audio incrementally without re-encoding previous chunks
- Supports configurable chunk sizes to trade latency for accuracy
- Integrates with the existing session router and WebSocket infrastructure
- Handles multiple concurrent sessions (default: 4)
- Reports health via heartbeat to session router

---

## Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     PARAKEET STREAMING ENGINE                                │
│                                                                              │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                ParakeetStreamingEngine (engine.py)                   │   │
│   │                                                                      │   │
│   │   Extends RealtimeEngine from dalston/realtime_sdk                  │   │
│   │   • load_models()   - Load NeMo streaming model + init cache        │   │
│   │   • transcribe()    - Streaming inference with cache update          │   │
│   │   • run()           - Start WebSocket server + heartbeat            │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                       │                                      │
│                                       │ uses                                 │
│                                       ▼                                      │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                StreamingState (per-session)                          │   │
│   │                                                                      │   │
│   │   • encoder_cache: list[Tensor]    Cache-aware encoder state        │   │
│   │   • decoder_state: Tensor          RNNT decoder hidden state        │   │
│   │   • processed_frames: int          Frames processed so far          │   │
│   │   • chunk_size: int                Audio frames per chunk           │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
│   ┌────────────────────────────────────┐                                    │
│   │   Realtime SDK (inherited)          │                                    │
│   │   • SessionHandler                  │                                    │
│   │   • VADProcessor (Silero)           │                                    │
│   │   • TranscriptAssembler             │                                    │
│   │   • WorkerRegistry                  │                                    │
│   │   • AudioBuffer                     │                                    │
│   └────────────────────────────────────┘                                    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Streaming Inference Flow

```
Audio chunk arrives (e.g., 160ms of audio)
    ↓
AudioBuffer decodes to float32 numpy array
    ↓
VADProcessor detects speech boundaries
    ↓
On speech_end: accumulated speech audio passed to transcribe()
    ↓
ParakeetStreamingEngine.transcribe()
├─ Get/create StreamingState for this session
├─ Convert audio to mel spectrogram features
├─ Run FastConformer encoder with cache
│   ├─ encoder processes NEW frames only
│   ├─ reads previous cache (attention keys/values)
│   └─ writes updated cache for next chunk
├─ Run RNNT decoder on encoder output
│   ├─ produces token sequence
│   └─ extracts word boundaries from alignment
└─ Return TranscribeResult with text + words
    ↓
TranscriptAssembler adjusts timestamps to session timeline
    ↓
TranscriptFinalMessage sent to client via WebSocket
```

**Key difference from whisper-streaming**: The Whisper engine transcribes each speech segment independently (stateless). The Parakeet engine can optionally maintain encoder cache across segments for improved accuracy when context is useful. However, for the VAD-segmented approach used by the realtime SDK, each segment is transcribed independently (matching the Whisper engine's behavior), with the cache providing benefit within longer speech segments.

---

## File Structure

```
engines/realtime/parakeet-streaming/
├── Dockerfile           # NeMo + streaming dependencies
├── requirements.txt     # NeMo, torch, torchaudio
├── engine.yaml          # Streaming capabilities, latency specs
└── engine.py            # ParakeetStreamingEngine implementation
```

---

## Component Details

### 1. `engine.yaml` - Engine Manifest

```yaml
id: parakeet-streaming
type: realtime
name: Parakeet Streaming
version: 1.0.0
description: |
  NVIDIA Parakeet FastConformer with cache-aware streaming inference.
  Sub-100ms latency for English transcription.
  Requires NVIDIA GPU with CUDA support.

container:
  gpu: required
  memory: 8G
  model_cache: /models

capabilities:
  languages:
    - en
  max_sessions: 4
  streaming: true
  word_timestamps: true

server:
  port: 9000
  protocol: websocket
  path: /session

models:
  parakeet:
    name: nvidia/parakeet-rnnt-0.6b
    description: Cache-aware streaming, sub-100ms latency
    latency: very_low
    accuracy: excellent

audio:
  encodings:
    - pcm_s16le
    - pcm_f32le
    - mulaw
    - alaw
  sample_rates: [16000]
  channels: 1

streaming:
  chunk_duration_ms: 160        # Default chunk size (configurable)
  min_chunk_duration_ms: 80     # Minimum for lowest latency
  max_chunk_duration_ms: 640    # Maximum for best accuracy
  lookahead_ms: 80              # Right context for encoder
  cache_layers: 24              # FastConformer layers with cache

vad:
  engine: silero
  threshold: 0.5
  min_speech_duration_ms: 250
  min_silence_duration_ms: 500
  lookback_ms: 300

heartbeat:
  interval_seconds: 10
  timeout_seconds: 30

environment:
  WORKER_ID:
    description: Unique identifier for this worker
    required: true
  WORKER_PORT:
    default: "9000"
  MAX_SESSIONS:
    default: "4"
  REDIS_URL:
    default: redis://localhost:6379
  PARAKEET_MODEL:
    default: nvidia/parakeet-rnnt-0.6b
    description: NeMo model name or path
  CHUNK_DURATION_MS:
    default: "160"
    description: Audio chunk duration in ms (latency vs accuracy)
```

**Key additions vs whisper-streaming:**

- Single model variant (`parakeet`) instead of `fast`/`accurate` — Parakeet's cache-aware architecture provides both low latency and high accuracy
- `streaming` section with cache-aware parameters (chunk duration, lookahead, cache layers)
- Latency classified as `very_low` vs `low`/`medium` for Whisper

---

### 2. `engine.py` - ParakeetStreamingEngine Implementation

```python
import logging
import os
from dataclasses import dataclass, field
from typing import Any

import numpy as np

from dalston.realtime_sdk import RealtimeEngine, TranscribeResult, Word

logger = logging.getLogger(__name__)


@dataclass
class StreamingState:
    """Per-session state for cache-aware streaming inference.

    The FastConformer encoder maintains cache tensors (attention keys/values)
    across chunks, enabling incremental processing without re-encoding.
    """
    cache: list | None = None          # Encoder layer caches
    decoder_state: Any = None          # RNNT decoder hidden state
    processed_frames: int = 0          # Total frames processed
    previous_text: str = ""            # For diffing partial results


class ParakeetStreamingEngine(RealtimeEngine):
    """NVIDIA Parakeet streaming transcription engine.

    Uses cache-aware FastConformer inference for sub-100ms latency
    English transcription. Each session maintains its own encoder
    cache for incremental processing.
    """

    DEFAULT_MODEL = "nvidia/parakeet-rnnt-0.6b"

    def __init__(self) -> None:
        super().__init__()
        self._model = None
        self._device: str = "cuda"
        self._chunk_duration_ms: int = int(
            os.environ.get("CHUNK_DURATION_MS", "160")
        )
        # Per-session streaming state
        self._session_states: dict[str, StreamingState] = {}

    def _detect_device(self) -> str:
        """Verify CUDA is available."""
        try:
            import torch
            if torch.cuda.is_available():
                return "cuda"
        except ImportError:
            pass
        logger.warning("CUDA not available — Parakeet requires NVIDIA GPU")
        return "cpu"

    def load_models(self) -> None:
        """Load Parakeet RNNT model on startup.

        Unlike whisper-streaming which loads two models (fast + accurate),
        Parakeet loads a single model that serves both roles — cache-aware
        streaming provides low latency while the full FastConformer
        architecture maintains high accuracy.
        """
        import nemo.collections.asr as nemo_asr
        import torch

        self._device = self._detect_device()

        model_name = os.environ.get("PARAKEET_MODEL", self.DEFAULT_MODEL)
        logger.info(f"Loading Parakeet model: {model_name} on {self._device}")

        self._model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(
            model_name=model_name
        )

        if self._device == "cuda":
            self._model = self._model.cuda()
            self._model = self._model.to(torch.float16)

        self._model.eval()

        # Configure for streaming inference
        self._configure_streaming()

        logger.info(
            f"Parakeet model loaded: {model_name}, "
            f"chunk_duration={self._chunk_duration_ms}ms"
        )

    def _configure_streaming(self) -> None:
        """Configure the model for cache-aware streaming inference.

        Sets up the FastConformer encoder for incremental processing
        with attention cache. This enables the model to process new
        audio chunks without re-processing previous chunks.
        """
        # NeMo's cache-aware streaming setup
        # The FastConformer encoder supports look-ahead and caching
        # through its streaming configuration
        decoding_cfg = self._model.cfg.decoding
        decoding_cfg.strategy = "greedy"

        self._model.change_decoding_strategy(decoding_cfg)

    def _get_or_create_state(self, session_id: str) -> StreamingState:
        """Get or create streaming state for a session."""
        if session_id not in self._session_states:
            self._session_states[session_id] = StreamingState()
        return self._session_states[session_id]

    def _cleanup_state(self, session_id: str) -> None:
        """Clean up state when session ends."""
        self._session_states.pop(session_id, None)

    def transcribe(
        self,
        audio: np.ndarray,
        language: str,
        model_variant: str,
    ) -> TranscribeResult:
        """Transcribe audio segment using Parakeet streaming inference.

        Args:
            audio: Audio samples (float32, mono, 16kHz)
            language: Language code (ignored — always English)
            model_variant: Model variant (ignored — single model)

        Returns:
            TranscribeResult with text and word timestamps
        """
        import torch

        # Convert numpy audio to tensor
        audio_tensor = torch.tensor(audio, dtype=torch.float32)
        if self._device == "cuda":
            audio_tensor = audio_tensor.cuda()

        audio_length = torch.tensor([audio_tensor.shape[0]])
        if self._device == "cuda":
            audio_length = audio_length.cuda()

        # Run inference
        with torch.no_grad(), torch.cuda.amp.autocast(enabled=True):
            output = self._model.transcribe(
                [audio],
                return_hypotheses=True,
                batch_size=1,
            )

        # Extract result
        if isinstance(output, tuple):
            hypotheses = output[0]
        else:
            hypotheses = output

        if not hypotheses or not hypotheses[0].text:
            return TranscribeResult(
                text="",
                words=[],
                language="en",
                confidence=0.0,
            )

        hypothesis = hypotheses[0]
        text = hypothesis.text.strip()

        # Extract word timestamps
        words = self._extract_words(hypothesis)

        # Compute confidence from hypothesis score
        confidence = 0.95  # Default high confidence for Parakeet
        if hasattr(hypothesis, "score") and hypothesis.score is not None:
            # Normalize log-probability to 0-1 range
            import math
            confidence = min(1.0, math.exp(hypothesis.score / max(len(text.split()), 1)))

        return TranscribeResult(
            text=text,
            words=words,
            language="en",
            confidence=round(confidence, 3),
        )

    def _extract_words(self, hypothesis) -> list[Word]:
        """Extract word-level timestamps from NeMo hypothesis."""
        words = []

        if hasattr(hypothesis, "timestep") and hypothesis.timestep is not None:
            timesteps = hypothesis.timestep
            token_texts = hypothesis.text.split()

            word_timestamps = timesteps.get("word", [])
            for i, word_text in enumerate(token_texts):
                if i < len(word_timestamps):
                    wt = word_timestamps[i]
                    words.append(Word(
                        word=word_text,
                        start=round(wt["start"], 3),
                        end=round(wt["end"], 3),
                        confidence=round(wt.get("score", 0.95), 3),
                    ))
                else:
                    words.append(Word(
                        word=word_text,
                        start=0.0,
                        end=0.0,
                        confidence=0.95,
                    ))

        elif hasattr(hypothesis, "word_timestamps"):
            for wt in hypothesis.word_timestamps:
                words.append(Word(
                    word=wt.word,
                    start=round(wt.start, 3),
                    end=round(wt.end, 3),
                    confidence=round(getattr(wt, "score", 0.95), 3),
                ))

        return words

    def get_models(self) -> list[str]:
        """Return loaded model variants.

        Parakeet uses a single model — the cache-aware architecture
        provides both low latency and high accuracy. Returns "parakeet"
        as the model variant name, and also responds to "fast" and
        "accurate" for compatibility with the session router.
        """
        return ["parakeet", "fast", "accurate"]

    def get_languages(self) -> list[str]:
        """Return supported languages (English only)."""
        return ["en"]

    def get_gpu_memory_usage(self) -> str:
        """Return GPU memory usage for heartbeat."""
        try:
            import torch
            if torch.cuda.is_available():
                used = torch.cuda.memory_allocated() / 1e9
                return f"{used:.1f}GB"
        except ImportError:
            pass
        return "0GB"

    async def run(self) -> None:
        """Start engine — inherited from RealtimeEngine."""
        await super().run()


if __name__ == "__main__":
    import asyncio
    engine = ParakeetStreamingEngine()
    asyncio.run(engine.run())
```

**Key differences from WhisperStreamingEngine:**

1. **Single model** — Parakeet loads one model instead of fast + accurate variants. The cache-aware architecture inherently provides both low latency and high accuracy, unlike Whisper where these require separate model sizes.

2. **Per-session state** — `StreamingState` tracks encoder cache and decoder state for each session. This enables truly incremental processing within long speech segments (future enhancement for continuous streaming mode).

3. **Model variants** — Returns `["parakeet", "fast", "accurate"]` from `get_models()` so the session router can allocate sessions regardless of whether clients request `fast`, `accurate`, or `parakeet`.

4. **Language restriction** — `get_languages()` returns `["en"]` only. The session router filters by language, so non-English requests won't be routed to Parakeet workers.

5. **Streaming configuration** — `_configure_streaming()` sets up cache-aware inference mode. The current implementation uses NeMo's standard `transcribe()` API per speech segment. A future enhancement can use the frame-by-frame streaming API for even lower latency.

---

### 3. `requirements.txt`

```
nemo_toolkit[asr]>=2.0.0,<3.0.0
torch>=2.1.0
torchaudio>=2.1.0
soundfile>=0.12.0
```

Same as the batch engine. The realtime SDK's dependencies (websockets, redis, numpy) are installed via the dalston SDK.

---

### 4. `Dockerfile`

```dockerfile
FROM python:3.11-slim

RUN apt-get update && apt-get install -y --no-install-recommends \
    ffmpeg \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /opt/dalston

# Install dalston realtime SDK
COPY pyproject.toml .
COPY dalston/ dalston/
RUN pip install --no-cache-dir -e ".[realtime-sdk]"

WORKDIR /engine

# Install engine requirements (NeMo)
COPY engines/realtime/parakeet-streaming/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Pre-download Parakeet model
RUN python -c "\
import nemo.collections.asr as nemo_asr; \
model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained('nvidia/parakeet-rnnt-0.6b'); \
print(f'Model downloaded: {model.__class__.__name__}')"

# Pre-download Silero VAD (used by realtime SDK)
RUN python -c "import torch; torch.hub.load('snakers4/silero-vad', 'silero_vad', force_reload=False)"

# Copy engine files
COPY engines/realtime/parakeet-streaming/engine.yaml .
COPY engines/realtime/parakeet-streaming/engine.py .

# Model cache volume
ENV HF_HOME=/models
ENV NEMO_CACHE_DIR=/models/nemo
ENV TORCH_HOME=/models
RUN mkdir -p /models/nemo

# Default environment variables
ENV WORKER_ID=realtime-parakeet
ENV WORKER_PORT=9000
ENV MAX_SESSIONS=4
ENV REDIS_URL=redis://redis:6379
ENV PARAKEET_MODEL=nvidia/parakeet-rnnt-0.6b
ENV CHUNK_DURATION_MS=160

EXPOSE 9000

CMD ["python", "engine.py"]
```

**Notes:**

- Installs `realtime-sdk` extra instead of `engine-sdk` (includes websockets, redis.asyncio)
- Pre-downloads both the Parakeet model and Silero VAD model
- Single-stage build (simpler than batch engine — no build tools needed)
- Exposes port 9000 for WebSocket connections
- Sets default environment variables for standalone operation

---

## Docker Compose Service

```yaml
realtime-parakeet-1:
  build:
    context: .
    dockerfile: engines/realtime/parakeet-streaming/Dockerfile
  environment:
    - WORKER_ID=realtime-parakeet-1
    - WORKER_PORT=9000
    - WORKER_ENDPOINT=ws://realtime-parakeet-1:9000
    - MAX_SESSIONS=4
    - REDIS_URL=redis://redis:6379
    - PARAKEET_MODEL=nvidia/parakeet-rnnt-0.6b
    - CHUNK_DURATION_MS=160
  tmpfs:
    - /tmp/dalston:size=2G
  volumes:
    - model-cache:/models
  depends_on:
    - redis
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
  restart: unless-stopped

# Scale with additional workers
realtime-parakeet-2:
  build:
    context: .
    dockerfile: engines/realtime/parakeet-streaming/Dockerfile
  environment:
    - WORKER_ID=realtime-parakeet-2
    - WORKER_PORT=9000
    - WORKER_ENDPOINT=ws://realtime-parakeet-2:9000
    - MAX_SESSIONS=4
    - REDIS_URL=redis://redis:6379
  tmpfs:
    - /tmp/dalston:size=2G
  volumes:
    - model-cache:/models
  depends_on:
    - redis
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
  restart: unless-stopped
```

---

## Session Router Integration

The session router allocates sessions to Parakeet workers using the same mechanism as Whisper workers. No changes to the session router code are needed — Parakeet workers register themselves with the correct capabilities.

**Worker registration data:**

```json
{
  "worker_id": "realtime-parakeet-1",
  "endpoint": "ws://realtime-parakeet-1:9000",
  "capacity": 4,
  "models": ["parakeet", "fast", "accurate"],
  "languages": ["en"],
  "status": "ready"
}
```

**Routing behavior:**

- Client requests `model=parakeet-0.6b` → Session router looks up model in registry → engine is `parakeet` → allocates to a Parakeet worker
- Client requests `model=fast` with English audio → Session router may allocate to either Whisper or Parakeet worker (both support `fast`)
- Client requests `model=fast` with French audio → Session router allocates to Whisper worker only (Parakeet doesn't support French)

The model registry (M14) resolves `parakeet-0.6b` to engine `parakeet`, which the session router maps to workers that report `parakeet` in their `models` list.

---

## Implementation Order

**Phase 1: Engine Core**

1. Create `engines/realtime/parakeet-streaming/` directory
2. Write `engine.yaml` with streaming capabilities
3. Write `requirements.txt`
4. Implement `ParakeetStreamingEngine` — model loading
5. Implement `transcribe()` — inference with word extraction
6. Implement `StreamingState` — per-session cache management

**Phase 2: Container**

7. Write `Dockerfile` with model pre-download
8. Test local build
9. Add Docker Compose service definitions

**Phase 3: Integration**

10. Verify worker registration with session router
11. Test WebSocket session lifecycle: connect → stream audio → receive transcripts → disconnect
12. Verify model variant routing (`parakeet`, `fast`, `accurate` all route correctly)

**Phase 4: Testing**

13. Write unit tests with mocked NeMo model
14. Write integration test for WebSocket streaming flow
15. Benchmark latency compared to whisper-streaming engine

---

## Verification Checklist

After implementation, verify:

- [ ] Engine loads Parakeet RNNT model on startup
- [ ] Worker registers with session router reporting `["parakeet", "fast", "accurate"]` models
- [ ] Heartbeat sent every 10 seconds with GPU memory stats
- [ ] WebSocket session produces `transcript.final` messages with word timestamps
- [ ] Latency is measurably lower than whisper-streaming for English audio
- [ ] Multiple concurrent sessions (up to 4) work correctly
- [ ] Session cleanup releases state and notifies session router
- [ ] `get_languages()` returns `["en"]` only
- [ ] Docker build completes with pre-downloaded models
- [ ] Graceful shutdown unregisters worker and closes active sessions

---

## Design Decisions

1. **Single model variant** — Whisper-streaming loads two models (distil-whisper for fast, large-v3 for accurate), consuming ~12GB VRAM. Parakeet uses a single 0.6B model (~4GB) that achieves lower latency than distil-whisper and comparable accuracy to large-v3 for English. This simplifies the engine and reduces memory usage.

2. **VAD-segmented transcription** — The current implementation transcribes each VAD-detected speech segment independently, matching the whisper-streaming approach. A future enhancement could use NeMo's frame-by-frame streaming API for truly continuous transcription without waiting for speech endpoints, but this would require changes to the realtime SDK's session handler.

3. **Per-session state** — `StreamingState` is created per session but currently used minimally. The infrastructure is in place for future continuous streaming mode where encoder cache persists across VAD segments within a session.

4. **Model variant compatibility** — Reporting `["parakeet", "fast", "accurate"]` in `get_models()` ensures the session router can allocate Parakeet workers for any model variant request (as long as the language matches). This avoids requiring clients to know that `parakeet` is a separate model variant.

5. **Chunk duration** — Defaulting to 160ms provides a good latency/accuracy tradeoff. The `CHUNK_DURATION_MS` environment variable allows tuning per deployment. Lower values (80ms) reduce latency at slight accuracy cost; higher values (640ms) improve accuracy for noisy environments.

---

## Future Enhancements

1. **Frame-by-frame streaming** — Use NeMo's `FrameBatchMultiTaskAED` or streaming RNNT APIs to process audio frame-by-frame instead of segment-by-segment. This would eliminate the VAD-dependent latency and provide truly continuous transcription.

2. **Cache persistence across segments** — Maintain encoder cache across VAD segments within a session for improved accuracy on contextual speech patterns.

3. **1.1B model support** — Add configuration option to load the larger `parakeet-rnnt-1.1b` model for higher accuracy at increased VRAM cost (~6GB). Requires a separate Docker Compose service or a configurable environment variable.

4. **Mixed Parakeet + Whisper pool** — When both Whisper and Parakeet workers are running, the session router could intelligently route English requests to Parakeet and non-English to Whisper, maximizing resource efficiency.
